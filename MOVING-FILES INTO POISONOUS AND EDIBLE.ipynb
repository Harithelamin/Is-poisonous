{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df75c37-9285-454a-85b0-820eddb6dd6e",
   "metadata": {},
   "source": [
    "###This contains the list of directories used\n",
    "Dataset 1 https://www.kaggle.com/datasets/marcosvolpato/edible-and-poisonous-fungi 269MB\n",
    "The dataset is separated in 2 classes , mushroom sporocarps and not mushroom sporocarps. Each class has 2 subclasses, edible and non-edible(includes medicinal, unpalatable and hallucinogenic fungi).\n",
    "\n",
    "Dataset 2 https://www.kaggle.com/datasets/mdhasanahmad/12-mushroom-species-dataset 111MB\n",
    "This dataset comprises images of 12 distinct species of mushrooms, collected from various sources, primarily sourced from Mushroom World. The species included are Agaricus, Amanita, Boletus, Cortinarius, Entoloma, Exidia, Hygrocybe, Inocybe, Lactarius, Pluteus, Russula, and Suillus, representing a diverse array of fungal taxa.\n",
    "\n",
    "Architecture Explanation and Rationale:\r\n",
    "Residual Blocks:\r\n",
    "\r\n",
    "Feature Hierarchies: Residual blocks are known for promoting feature hierarchies by facilitating the learning of complex patterns at different levels of abstraction. This is crucial for image classification tasks where distinguishing between subtle visual cues is important.\r\n",
    "Gradient Flow: Residual connections mitigate the vanishing gradient problem in deep networks, enabling smoother gradient flow during training. This is particularly beneficial for deep architectures, ensuring more stable and effective optimization.\r\n",
    "Dense Blocks:\r\n",
    "\r\n",
    "Feature Reuse: Dense blocks encourage feature reuse through dense connections, where each layer receives inputs from all preceding layers. This can enhance the network's representational power and robustness, capturing intricate features important for classification.\r\n",
    "Complex Pattern Learning: Mushroom classification may require the model to learn complex patterns and relationships among different parts of the mushroom images. Dense blocks excel in capturing such dependencies and correlations within the data.\r\n",
    "Hybrid Design:\r\n",
    "\r\n",
    "Complementary Strengths: The hybrid design leverages the complementary strengths of residual and dense blocks. While residual blocks focus on depth and hierarchical feature learning, dense blocks emphasize feature reuse and intricate pattern detection.\r\n",
    "Effective Feature Extraction: The interleaving of these blocks allows the model to effectively extract features at multiple levels of granularity, from basic edges and textures to more abstract structures indicative of poisonous or non-poisonous mushrooms.\r\n",
    "Global Average Pooling and Dense Layer:\r\n",
    "\r\n",
    "Dimension Reduction: Applying global average pooling before the final dense layer reduces spatial dimensions and aggregates feature information from across the entire image. This helps in focusing on the most discriminative features for classification.\r\n",
    "Binary Classification Output: The final dense layer with softmax activation outputs probabilities for the two classes (poisonous and non-poisonous), making it suitable for binary classification tasks.\r\n",
    "Potential Benefits and Expectations:\r\n",
    "Robust Feature Learning: The proposed architecture is designed to robustly learn features from mushroom images, capturing both local and global patterns essential for classification.\r\n",
    "\r\n",
    "Improved Gradient Flow: The combination of residual and dense blocks promotes better gradient flow and information propagation, facilitating effective training and convergence.\r\n",
    "\r\n",
    "Hierarchical Representation: By learning hierarchical representations through stacked blocks, the model can discern important features at different levels of complexity, aiding in accurate classification.\r\n",
    "\r\n",
    "Adaptability to Complex Data: Mushrooms can exhibit varied visual characteristics even within the same class (e.g., poisonous mushrooms can have diverse appearances). The hybrid architecture's flexibility and capacity to learn intricate details make it well-suited for such datasets.\r\n",
    "\r\n",
    "Conclusion:\r\n",
    "In summary, the choice of a hybrid architecture combining residual and dense blocks for mushroom classification is motivated by its ability to handle complex visual data effectively. This architecture offers a balance between depth, feature reuse, and gradient stability, which are crucial for achieving high performance in binary image classification tasks like distinguishing between poisonous and non-poisonous mushrooms. Experimentation with this architecture, along with appropriate tuning and optimization, is expected to yield promising results for the given classification problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "70b3095f-e832-4378-b74e-542b939924cb",
   "metadata": {},
   "source": [
    "Types of mushroom in Norway. \n",
    "EDIBLE:\n",
    "1. Boletus-edulis(Steinsopp)\n",
    "2. Tylopilus felleus(Gallerørsopp)\n",
    "3. Cantharellus cibarius(Kantarell)\n",
    "4. Craterellus tubaeformis(Traktkantarell)\n",
    "5. Hydnum repandum - Wood Hedgehog(Blek piggsopp)\n",
    "6. Hydnum rufescens - Terracotta Hedgehog (rødgul piggsopp)\n",
    "7. Albatrellus ovinus (Fåresopp)\n",
    "8. Albatrellus confluens (Franskbrødsopp)\n",
    "9. Coprinus comatus ( Matblekksopp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7966b28-93e8-49db-8910-f6b0c0de590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_images_to_parent_folders(main_folder):\n",
    "    # Traverse all directories and subdirectories within the main folder\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        for file in files:\n",
    "            if is_image_file(file):\n",
    "                # Determine the parent directory (subfolder) of the current file\n",
    "                parent_folder = os.path.basename(root)\n",
    "\n",
    "                # Construct paths for source (current file) and target (parent folder)\n",
    "                source_path = os.path.join(root, file)\n",
    "                target_folder = os.path.dirname(root)  # Get the parent directory path\n",
    "\n",
    "                # Move the image file to the target (parent) folder\n",
    "                target_path = os.path.join(target_folder, file)\n",
    "                shutil.move(source_path, target_path)\n",
    "                print(f\"Moved {file} to {target_path}\")\n",
    "\n",
    "def is_image_file(filename):\n",
    "    # Check if the file has a valid image extension\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff')\n",
    "    return filename.lower().endswith(valid_extensions)\n",
    "\n",
    "# Define the main folder containing subfolders with sub-subfolders (images)\n",
    "main_folder = r'E:\\Large Mushroom Dataset\\mushroom_dataset\\Classes\\LETS USE ONLY THIS'\n",
    "\n",
    "# Call the function to move images to their parent folders\n",
    "move_images_to_parent_folders(main_folder)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52e67c8b-c41b-4a07-b493-e716ead97f8e",
   "metadata": {},
   "source": [
    "Creating a flow diagram to illustrate the architecture involving inception modules and residual blocks can be very helpful for understanding the structure of the neural network. Let's break down the diagram step by step.\n",
    "\n",
    "Architecture Overview:\n",
    "\n",
    "Input Layer\n",
    "Initial Convolutional Layer\n",
    "Max Pooling Layer\n",
    "Then the architecture uses a series of Inception Modules followed by Residual Blocks:\n",
    "\n",
    "Inception Module 1\n",
    "Residual Block\n",
    "Inception Module 2\n",
    "Residual Block\n",
    "Inception Module 3\n",
    "Residual Block\n",
    "Finally, the architecture concludes with:\n",
    "\n",
    "Global Average Pooling\n",
    "Fully Connected Layer\n",
    "Dropout Layer\n",
    "Output Layer\n",
    "Detailed Flow Diagram:\n",
    "\n",
    "Input\n",
    "  |\n",
    "[Conv2D: 64 filters, 7x7 kernel, stride 2, padding=same]\n",
    "  |\n",
    "[MaxPooling2D: 3x3 pool size, stride 2, padding=same]\n",
    "  |\n",
    "  |-----------------------|\n",
    "  |                       |\n",
    "Inception Module 1       Residual Block 1\n",
    "[Branch 1: 1x1 Conv]     [Conv2D: 3x3, 256 filters]\n",
    "[Branch 2: 3x3 Conv]     [BatchNorm, ReLU]\n",
    "[Branch 3: 5x5 Conv]     [Conv2D: 3x3, 256 filters]\n",
    "[Branch 4: MaxPool]      [BatchNorm]\n",
    "  |                       [Add (Residual Connection)]\n",
    "  |-----------------------| \n",
    "  |\n",
    "Inception Module 2\n",
    "[Branch 1: 1x1 Conv]\n",
    "[Branch 2: 3x3 Conv]\n",
    "[Branch 3: 5x5 Conv]\n",
    "[Branch 4: MaxPool]\n",
    "  |\n",
    "  |-----------------------|\n",
    "  |                       |\n",
    "Residual Block 2         Inception Module 3\n",
    "[Conv2D: 3x3, 384 filters] [Branch 1: 1x1 Conv]\n",
    "[BatchNorm, ReLU]         [Branch 2: 3x3 Conv]\n",
    "[Conv2D: 3x3, 384 filters] [Branch 3: 5x5 Conv]\n",
    "[BatchNorm]               [Branch 4: MaxPool]\n",
    "[Add (Residual Connection)]|\n",
    "  |\n",
    "Global Average Pooling\n",
    "  |\n",
    "[Dense: 512 units, ReLU]\n",
    "  |\n",
    "[Dropout: 0.5]\n",
    "  |\n",
    "[Dense: num_classes, softmax]\n",
    "\n",
    "Output\n",
    "\n",
    "Notes:\n",
    "\n",
    "Each block (Conv2D, MaxPooling2D, etc.) in the diagram represents a layer with specified parameters such as filter size, kernel size, padding, and activation function.\n",
    "The flow diagram visually represents the sequence of operations starting from the input layer through each component of the model, including inception modules, residual blocks, and concluding with the output layer.\n",
    "The notation used (e.g., [Conv2D: 64 filters, 7x7 kernel, stride 2, padding=same]) helps in clearly indicating the configuration of each layer/component within the architecture.\n",
    "This detailed flow diagram should provide a clearer visualization of how data flows through the custom model architecture, including all the specified details such as kernel sizes, filter numbers, and residual connections. Adjustments can be made to this representation based on specific preferences or additional details you want to emphasize.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6ee4935-f90b-46bf-a78a-843d71d425e6",
   "metadata": {},
   "source": [
    "STRENGTH: \n",
    "The architecture described, which combines elements of both Inception modules and residual blocks, is designed to leverage the strengths of each component to potentially outperform simpler architectures. Here's how and why this architecture might work better:\n",
    "\n",
    "Diverse Feature Extraction with Inception Modules:\n",
    "\n",
    "Inception modules utilize parallel convolutional branches (1x1, 3x3, 5x5 convolutions, and max pooling) to capture features at multiple scales within the same layer.\n",
    "This allows the network to extract diverse and rich features from the input data, enhancing its ability to learn complex patterns and representations.\n",
    "Efficient Parameter Utilization:\n",
    "\n",
    "The use of 1x1 convolutions in Inception modules helps reduce the number of parameters by performing dimensionality reduction before applying more computationally expensive operations (like 3x3 or 5x5 convolutions).\n",
    "This parameter efficiency can lead to better utilization of network capacity and potentially improved generalization performance.\n",
    "Residual Connections for Deeper Networks:\n",
    "\n",
    "Residual blocks enable the training of very deep networks by mitigating the vanishing gradient problem.\n",
    "The skip connections in residual blocks facilitate the flow of gradients during backpropagation, allowing for easier optimization of very deep networks.\n",
    "Deeper networks can potentially capture more abstract and hierarchical features, leading to better performance on complex tasks.\n",
    "Combining Strengths of Both Architectures:\n",
    "\n",
    "By integrating Inception modules with residual blocks, this architecture combines the strengths of feature diversification (Inception) and improved gradient flow in deep networks (residual connections).\n",
    "This hybrid approach aims to address limitations of earlier architectures (like plain convolutional networks) by offering a more flexible and robust framework for feature learning.\n",
    "Regularization and Training Stability:\n",
    "\n",
    "The inclusion of batch normalization in both Inception modules and residual blocks helps in stabilizing the training process by reducing internal covariate shift.\n",
    "Additionally, dropout is applied before the final fully connected layer to prevent overfitting and improve model generalization.\n",
    "Global Average Pooling for Spatial Features:\n",
    "\n",
    "The use of global average pooling before the fully connected layer encourages the network to focus on important spatial features rather than specific details, which can improve robustness to spatial translations and distortions.\n",
    "Overall, this architecture is designed to enhance feature learning, optimize parameter usage, facilitate training of deep networks, and improve generalization performance compared to simpler architectures. By integrating these advanced techniques, the model aims to excel in tasks that require learning complex patterns from high-dimensional data, such as image classification or object detection. However, the effectiveness of any architecture ultimately depends on the specific dataset and task at hand, and empirical validation through experimentation is crucial for assessing its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c002509-8601-4804-ab74-a29bd001f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weakness:\n",
    "While the described architecture incorporating Inception modules and residual blocks has several strengths, there are also potential limitations and considerations that could impact its effectiveness or make it less suitable for certain scenarios:\n",
    "\n",
    "Complexity and Overhead:\n",
    "\n",
    "The combination of Inception modules and residual blocks introduces additional complexity to the network architecture.\n",
    "This increased complexity can result in higher computational overhead during training and inference, potentially requiring more resources (such as memory and processing power).\n",
    "Difficulty in Training and Optimization:\n",
    "\n",
    "Deep networks with complex architectures like this one can be more challenging to train effectively.\n",
    "Optimizing such models may require extensive hyperparameter tuning and careful initialization strategies to prevent issues like vanishing or exploding gradients.\n",
    "Potential for Overfitting:\n",
    "\n",
    "The use of a deep and complex architecture may increase the risk of overfitting, especially when applied to smaller datasets.\n",
    "Overfitting occurs when a model learns to perform well on the training data but fails to generalize to unseen data.\n",
    "Sensitivity to Hyperparameters:\n",
    "\n",
    "Architectures combining multiple components (e.g., Inception modules, residual blocks) often have multiple hyperparameters that need to be tuned.\n",
    "Finding the optimal set of hyperparameters (e.g., learning rates, batch sizes, regularization strengths) can be non-trivial and time-consuming.\n",
    "Limited Interpretability:\n",
    "\n",
    "Complex architectures can be less interpretable, making it harder to understand how the model arrives at its predictions.\n",
    "This lack of interpretability can be a drawback in certain applications where model transparency and explainability are important (e.g., healthcare or legal domains).\n",
    "Resource Intensive for Deployment:\n",
    "\n",
    "The computational demands of the architecture might make it less suitable for deployment on resource-constrained platforms (e.g., mobile devices or embedded systems).\n",
    "Task-Specific Suitability:\n",
    "\n",
    "Not all tasks may benefit from such a complex architecture. For simpler tasks or smaller datasets, a simpler model with fewer parameters might generalize better and be more efficient.\n",
    "Alternative Architectures:\n",
    "\n",
    "There are alternative architectures specifically designed for certain tasks (e.g., image segmentation, object detection) that may offer better performance or efficiency depending on the requirements.\n",
    "Empirical Validation Required:\n",
    "\n",
    "While the architecture incorporates advanced techniques, its effectiveness ultimately depends on empirical validation on specific datasets and tasks.\n",
    "What works well in theory may not always translate into superior performance in practice.\n",
    "In summary, while the described architecture combines powerful techniques for feature learning and network optimization, it is essential to carefully consider its complexity, training dynamics, potential for overfitting, and suitability for specific tasks before adopting it. Experimentation and benchmarking against simpler baselines or alternative architectures are crucial steps in evaluating the efficacy of such advanced models.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8b8693d-6887-47e5-8444-b59ac95db539",
   "metadata": {},
   "source": [
    "Classes of fruits and vegetables in imagent\n",
    "cheeseburger\n",
    "hotdog\n",
    "cabbalge\n",
    "broccoli\n",
    "cauliflower\n",
    "cucumber\n",
    "mushroom\n",
    "strawbery\n",
    "orange\n",
    "lemon\n",
    "pineapple\n",
    "banana\n",
    "jackfruit\n",
    "apple\n",
    "pomegranate\n",
    "pizza\n",
    "burrito\n",
    "expresso\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
